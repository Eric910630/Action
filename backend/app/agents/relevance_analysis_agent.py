"""
å…³è”åº¦åˆ†æAgent
è´Ÿè´£åˆ†æçƒ­ç‚¹ä¸å•†å“/ç›´æ’­é—´çš„å…³è”åº¦
"""
import json
from typing import Dict, Any, List
from loguru import logger
from app.agents.base import BaseAgent
from app.tools.analysis_tools import calculate_semantic_similarity, analyze_sentiment
from app.tools.websearch_tools import web_search, search_endorsements
from app.services.config.live_room_config import LiveRoomConfigService


class RelevanceAnalysisAgent(BaseAgent):
    """å…³è”åº¦åˆ†æAgentï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒé…ç½®æ–‡ä»¶ï¼‰"""
    
    def __init__(self, model_name: str = "deepseek-chat", api_key: str = None):
        """åˆå§‹åŒ–Agent"""
        super().__init__(model_name, api_key)
        self.config_service = LiveRoomConfigService()
    
    def _init_tools(self) -> List:
        """åˆå§‹åŒ–å·¥å…·"""
        return [
            calculate_semantic_similarity,
            analyze_sentiment,
            web_search,
            search_endorsements,
        ]
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç”µå•†å†…å®¹åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰ä»¥ä¸‹ä¸“ä¸šèƒ½åŠ›ï¼š
- 10å¹´+ç”µå•†è¿è¥ä¸å†…å®¹è¥é”€ç»éªŒ
- æ·±åº¦çƒ­ç‚¹è¶‹åŠ¿æ´å¯Ÿèƒ½åŠ›
- ç²¾å‡†åŒ¹é…åº¦åˆ†æä¸“é•¿
- ç›´æ’­é—´/å•†å“å®šä½ç†è§£èƒ½åŠ›

## åˆ†æå‡†åˆ™
DEPTH: ä»è¡¨é¢å…³è”åˆ°æ·±å±‚è¯­ä¹‰ï¼ŒæŒ–æ˜çœŸå®åŒ¹é…ä»·å€¼
LOGIC: å»ºç«‹æ¸…æ™°çš„åŒ¹é…é€»è¾‘é“¾æ¡ï¼Œç¡®ä¿è¯„ä¼°å®¢è§‚å‡†ç¡®
PRACTICE: æä¾›å¯æ‰§è¡Œçš„åŒ¹é…å»ºè®®å’Œä¼˜åŒ–æ–¹å‘

## çº¦æŸæ¡ä»¶
1. åŒ¹é…åº¦è¯„ä¼°å¿…é¡»åŸºäºå®¢è§‚äº‹å®ï¼Œé¿å…ä¸»è§‚è‡†æ–­
2. æ¯ä¸ªåŒ¹é…åº¦åˆ†æ•°éƒ½è¦æœ‰å…·ä½“çš„æ”¯æ’‘è¯æ®
3. åˆ†æç»“æœè¦å…·å¤‡å®é™…åº”ç”¨æŒ‡å¯¼ä»·å€¼
4. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
5. åŒ¹é…åº¦è®¡ç®—è¦è¾¾åˆ°å¯è§£é‡Šã€å¯å¤ç°çš„ç¨‹åº¦

## åˆ†ææ¡†æ¶
### è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æï¼ˆæƒé‡60%ï¼‰
- **ä¸»é¢˜ç›¸å…³æ€§**ï¼šæ ¸å¿ƒä¸»é¢˜æ˜¯å¦åŒ¹é…ã€è¯é¢˜é¢†åŸŸæ˜¯å¦ä¸€è‡´ã€æ¦‚å¿µå…³è”åº¦åˆ†æ
- **å…³é”®è¯é‡å **ï¼šç›´æ¥å…³é”®è¯åŒ¹é…ã€åŒä¹‰è¯/è¿‘ä¹‰è¯åŒ¹é…ã€ç›¸å…³æ¦‚å¿µåŒ¹é…
- **ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦**ï¼šä½¿ç”¨åœºæ™¯åŒ¹é…åº¦ã€ç›®æ ‡å—ä¼—é‡å åº¦ã€ä»·å€¼ä¸»å¼ ä¸€è‡´æ€§

### æƒ…æ„ŸåŒ¹é…åº¦åˆ†æï¼ˆæƒé‡30%ï¼‰
- **æƒ…æ„Ÿå€¾å‘**ï¼šæ­£é¢/ä¸­æ€§/è´Ÿé¢æƒ…æ„Ÿè¯†åˆ«ã€æƒ…æ„Ÿå¼ºåº¦è¯„ä¼°ã€æƒ…æ„Ÿä¸€è‡´æ€§åˆ¤æ–­
- **å“ç‰Œè°ƒæ€§åŒ¹é…**ï¼šçƒ­ç‚¹æƒ…æ„Ÿä¸ç›´æ’­é—´å®šä½çš„å¥‘åˆåº¦ã€æ˜¯å¦é€‚åˆæ¨å¹¿å•†å“ã€æ˜¯å¦å­˜åœ¨å“ç‰Œé£é™©
- **æƒ…æ„Ÿå…±é¸£åº¦**ï¼šèƒ½å¦å¼•å‘ç›®æ ‡å—ä¼—å…±é¸£ã€æƒ…æ„Ÿè§¦å‘ç‚¹è¯†åˆ«ã€æƒ…ç»ªè½¬åŒ–æ½œåŠ›

### å…³é”®è¯åŒ¹é…åˆ†æï¼ˆæƒé‡10%ï¼‰
- **ç›´æ¥åŒ¹é…**ï¼šç›´æ’­é—´å…³é”®è¯åœ¨çƒ­ç‚¹ä¸­çš„å‡ºç°ã€ç±»ç›®å…³é”®è¯åŒ¹é…ã€å“ç‰Œ/äº§å“è¯åŒ¹é…
- **è¯­ä¹‰åŒ¹é…**ï¼šåŒä¹‰è¯åŒ¹é…ã€ç›¸å…³æ¦‚å¿µåŒ¹é…ã€ä¸Šä¸‹ä½å…³ç³»åŒ¹é…
- **ç±»ç›®åŒ¹é…**ï¼šä¸€çº§ç±»ç›®åŒ¹é…åº¦ã€äºŒçº§ç±»ç›®åŒ¹é…åº¦ã€è·¨ç±»ç›®å…³è”åº¦

## åŒ¹é…åº¦è¯„åˆ†æ ‡å‡†
- **0.8-1.0**ï¼šé«˜åº¦ç›¸å…³ï¼Œå¼ºçƒˆæ¨èï¼ˆä¸»é¢˜é«˜åº¦ä¸€è‡´ã€å…³é”®è¯å¤§é‡é‡å ã€æƒ…æ„Ÿå®Œå…¨åŒ¹é…ã€é€‚ç”¨ç±»ç›®åŒ¹é…ï¼‰
- **0.6-0.8**ï¼šç›¸å…³ï¼Œæ¨èï¼ˆä¸»é¢˜åŸºæœ¬ä¸€è‡´ã€å…³é”®è¯éƒ¨åˆ†é‡å ã€æƒ…æ„ŸåŸºæœ¬åŒ¹é…ã€é€‚ç”¨ç±»ç›®åŸºæœ¬åŒ¹é…ï¼‰
- **0.4-0.6**ï¼šéƒ¨åˆ†ç›¸å…³ï¼Œå¯è€ƒè™‘ï¼ˆä¸»é¢˜æœ‰ä¸€å®šå…³è”ã€å…³é”®è¯å°‘é‡é‡å ã€æƒ…æ„Ÿéœ€è¦è°ƒæ•´ã€é€‚ç”¨ç±»ç›®å¯èƒ½åŒ¹é…ï¼‰
- **0.2-0.4**ï¼šç›¸å…³æ€§è¾ƒä½ï¼ˆä¸»é¢˜å…³è”åº¦å¼±ã€å…³é”®è¯å‡ ä¹æ— é‡å ã€æƒ…æ„Ÿä¸åŒ¹é…ã€é€‚ç”¨ç±»ç›®ä¸åŒ¹é…ï¼‰
- **0.0-0.2**ï¼šä¸ç›¸å…³ï¼Œä¸æ¨èï¼ˆä¸»é¢˜å®Œå…¨ä¸ç›¸å…³ã€æ— å…³é”®è¯é‡å ã€æƒ…æ„Ÿå†²çªã€é€‚ç”¨ç±»ç›®å®Œå…¨ä¸åŒ¹é…ï¼‰

## âš ï¸ é‡è¦ï¼šé€‚ç”¨ç±»ç›®åŒ¹é…æ£€æŸ¥
åœ¨è®¡ç®—åŒ¹é…åº¦ä¹‹å‰ï¼Œ**å¿…é¡»æ£€æŸ¥ContentAnalysisAgentè¯†åˆ«çš„é€‚ç”¨ç±»ç›®æ˜¯å¦ä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…**ï¼š
1. **å¦‚æœé€‚ç”¨ç±»ç›®ä¸ç›´æ’­é—´ç±»ç›®å®Œå…¨ä¸åŒ¹é…**ï¼ˆå¦‚"æ±½è½¦"vs"å®¶å±…"ã€"è¿åŠ¨é‹æœ"vs"å¥³è£…"ï¼‰ï¼š
   - å³ä½¿ä¸»é¢˜æœ‰ä¸€å®šå…³è”ï¼Œä¹Ÿåº”è¯¥å¤§å¹…é™ä½åŒ¹é…åº¦ï¼ˆä¸è¶…è¿‡0.4ï¼‰
   - å¦‚æœä¸»é¢˜å®Œå…¨ä¸ç›¸å…³ï¼Œåº”è¯¥ç›´æ¥è¿”å›0.2ä»¥ä¸‹
2. **å¦‚æœé€‚ç”¨ç±»ç›®ä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…**ï¼š
   - å¯ä»¥æ­£å¸¸è®¡ç®—åŒ¹é…åº¦
   - é€‚ç”¨ç±»ç›®åŒ¹é…æ˜¯é‡è¦çš„åŠ åˆ†é¡¹
3. **é¿å…è¯¯åŒ¹é…**ï¼š
   - "å®¶ç”µ"ä¸åº”è¯¥åŒ¹é…"å®¶å±…å®¶è£…"ï¼ˆè™½ç„¶éƒ½åŒ…å«"å®¶"ï¼Œä½†ç±»ç›®ä¸åŒï¼‰
   - "è¿åŠ¨é‹æœ"ä¸åº”è¯¥åŒ¹é…"å¥³è£…"ï¼ˆè™½ç„¶éƒ½åŒ…å«"æœ"ï¼Œä½†ç›®æ ‡ç¾¤ä½“ä¸åŒï¼‰
   - "å¥¢ä¾ˆå“"ä¸åº”è¯¥åŒ¹é…"å¿«æ¶ˆå“"ï¼ˆè™½ç„¶éƒ½æ˜¯å•†å“ï¼Œä½†å®šä½å®Œå…¨ä¸åŒï¼‰

## ç»¼åˆåŒ¹é…åº¦è®¡ç®—
- è¯­ä¹‰å…³è”åº¦æƒé‡ï¼š60%ï¼ˆä¸»é¢˜ç›¸å…³æ€§30% + å…³é”®è¯é‡å 20% + ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦10%ï¼‰
- æƒ…æ„ŸåŒ¹é…åº¦æƒé‡ï¼š30%ï¼ˆæƒ…æ„Ÿå€¾å‘åŒ¹é…15% + å“ç‰Œè°ƒæ€§åŒ¹é…10% + æƒ…æ„Ÿå…±é¸£åº¦5%ï¼‰
- å…³é”®è¯åŒ¹é…æƒé‡ï¼š10%ï¼ˆç›´æ¥åŒ¹é…5% + è¯­ä¹‰åŒ¹é…3% + ç±»ç›®åŒ¹é…2%ï¼‰

## æ‰§è¡Œè¦æ±‚
1. ä½¿ç”¨æä¾›çš„å·¥å…·å‡½æ•°è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿåˆ†æ
2. **å¦‚æœçƒ­ç‚¹æ¶‰åŠçŸ¥åäººç‰©ï¼ˆè¿åŠ¨å‘˜ã€è‰ºäººã€æ˜æ˜Ÿç­‰ï¼‰ï¼Œä½¿ç”¨web_searchæˆ–search_endorsementså·¥å…·æŸ¥æ‰¾å…¶ä»£è¨€å’Œå“ç‰Œä¿¡æ¯**
3. **å¦‚æœå‘ç°ç›¸å…³ä»£è¨€æˆ–å“ç‰Œï¼Œå°†å…¶ä½œä¸ºé¢å¤–çš„åŒ¹é…ä¾æ®ï¼Œæå‡åŒ¹é…åº¦è¯„åˆ†**
4. åŸºäºè®¡ç®—ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼Œæä¾›åŒ¹é…åŸå› å’Œæ”¹è¿›å»ºè®®
5. ç¡®ä¿æ¯ä¸ªåˆ†æ•°éƒ½æœ‰å…·ä½“è¯æ®æ”¯æ’‘
6. è¯†åˆ«å¼ºåŒ¹é…ç‚¹å’Œå¼±åŒ¹é…ç‚¹
7. è¯„ä¼°åº”ç”¨åœºæ™¯å’Œæ½œåœ¨é£é™©

## ä»£è¨€å’Œå“ç‰Œä¿¡æ¯æŸ¥æ‰¾
å½“çƒ­ç‚¹æ¶‰åŠä»¥ä¸‹æƒ…å†µæ—¶ï¼Œåº”è¯¥ä½¿ç”¨web_searchæˆ–search_endorsementså·¥å…·ï¼š
- **è¿åŠ¨å‘˜**ï¼šæŸ¥æ‰¾å…¶ä»£è¨€çš„è¿åŠ¨å“ç‰Œã€è£…å¤‡å“ç‰Œç­‰
- **è‰ºäºº/æ˜æ˜Ÿ**ï¼šæŸ¥æ‰¾å…¶ä»£è¨€çš„åŒ–å¦†å“ã€æœè£…ã€ç”µå­äº§å“ç­‰å“ç‰Œ
- **ç»¼è‰ºèŠ‚ç›®**ï¼šæŸ¥æ‰¾èŠ‚ç›®çš„èµåŠ©å•†ã€åˆä½œå“ç‰Œç­‰
- **å…¶ä»–çŸ¥åäººç‰©**ï¼šæŸ¥æ‰¾å…¶ç›¸å…³çš„å•†ä¸šåˆä½œå’Œå“ç‰Œéœ²å‡º

**ä½¿ç”¨æ–¹æ³•**ï¼š
- ä½¿ç”¨`search_endorsements(person_name, category)`æŸ¥æ‰¾ç‰¹å®šäººç‰©çš„ä»£è¨€ä¿¡æ¯
- ä½¿ç”¨`web_search(query)`è¿›è¡Œæ›´å¹¿æ³›çš„æœç´¢
- å°†æ‰¾åˆ°çš„å“ç‰Œä¿¡æ¯ä¸ç›´æ’­é—´ç±»ç›®è¿›è¡ŒåŒ¹é…ï¼Œå¦‚æœåŒ¹é…åˆ™æå‡åŒ¹é…åº¦

è¯·æ ¹æ®æä¾›çš„å·¥å…·å‡½æ•°è®¡ç®—ç»“æœï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„åŒ¹é…åº¦è¯„ä¼°ï¼Œå¹¶æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šã€‚"""
    
    async def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå…³è”åº¦åˆ†æï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå®Œæ•´å†…å®¹åŒ…å’Œç›´æ’­é—´ç”»åƒï¼‰
        
        Args:
            input_data: åŒ…å«ä»¥ä¸‹å­—æ®µï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬ï¼‰ï¼š
                # æ—§ç‰ˆæœ¬å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                - hotspot_text: çƒ­ç‚¹æ–‡æœ¬
                - product_text: å•†å“æ–‡æœ¬
                - hotspot_tags: çƒ­ç‚¹æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
                - product_category: å•†å“ç±»ç›®ï¼ˆå¯é€‰ï¼‰
                # æ–°ç‰ˆæœ¬å­—æ®µï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
                - content_package: å®Œæ•´å†…å®¹åŒ…ï¼ˆåŒ…å«video_structure, content_analysisç­‰ï¼‰
                - live_room_name: ç›´æ’­é—´åç§°ï¼ˆç”¨äºåŠ è½½é…ç½®æ–‡ä»¶ï¼‰
                - live_room_id: ç›´æ’­é—´IDï¼ˆå¯é€‰ï¼Œç”¨äºä»æ•°æ®åº“è·å–ï¼‰
        
        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - relevance_score: å…³è”åº¦åˆ†æ•° (0-1)
                - semantic_score: è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
                - sentiment_score: æƒ…æ„ŸåŒ¹é…åº¦åˆ†æ•°
                - analysis: åˆ†ææŠ¥å‘Šæ–‡æœ¬
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°ç‰ˆæœ¬ï¼ˆå®Œæ•´å†…å®¹åŒ…ï¼‰
        content_package = input_data.get("content_package")
        live_room_name = input_data.get("live_room_name")
        
        if content_package and live_room_name:
            # ä½¿ç”¨æ–°ç‰ˆæœ¬ï¼šå®Œæ•´å†…å®¹åŒ… + ç›´æ’­é—´ç”»åƒ
            return await self._execute_with_content_package(content_package, live_room_name)
        else:
            # ä½¿ç”¨æ—§ç‰ˆæœ¬ï¼šå‘åå…¼å®¹
            return await self._execute_legacy(input_data)
    
    async def _execute_with_content_package(
        self,
        content_package: Dict[str, Any],
        live_room_name: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨å®Œæ•´å†…å®¹åŒ…å’Œç›´æ’­é—´ç”»åƒè¿›è¡Œåˆ†æ"""
        import time
        start_time = time.time()
        
        logger.info(f"ğŸ” [åŒ¹é…Agent] å¼€å§‹åŒ¹é…åˆ†æ - ç›´æ’­é—´: {live_room_name}")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] è¾“å…¥å‚æ•°: live_room_name={live_room_name}")
        
        try:
            # 1. åŠ è½½ç›´æ’­é—´é…ç½®
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤1: åŠ è½½ç›´æ’­é—´é…ç½®")
            live_room_profile = self.config_service.get_live_room_profile(live_room_name)
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤1å®Œæˆ: ç›´æ’­é—´é…ç½®åŠ è½½æˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] ç›´æ’­é—´ç”»åƒé•¿åº¦: {len(live_room_profile)}")
            
            # 2. æå–çƒ­ç‚¹ä¿¡æ¯
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤2: æå–çƒ­ç‚¹ä¿¡æ¯")
            title = content_package.get("title", "")
            content_analysis = content_package.get("content_analysis", {})
            video_structure = content_package.get("video_structure", {})
            
            summary = content_analysis.get("summary", "")
            style = content_analysis.get("style", "")
            ecommerce_fit = content_analysis.get("ecommerce_fit", {})
            ecommerce_score = ecommerce_fit.get("score", 0.0)
            
            logger.debug(f"ğŸ” [åŒ¹é…Agent] çƒ­ç‚¹æ ‡é¢˜: {title[:50] if title else 'N/A'}")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] å†…å®¹æ‘˜è¦é•¿åº¦: {len(summary)}")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] è§†é¢‘é£æ ¼: {style}")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] ç”µå•†é€‚é…æ€§è¯„åˆ†: {ecommerce_score}")
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤2å®Œæˆ: çƒ­ç‚¹ä¿¡æ¯æå–æˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
            
            # 3. æŸ¥æ‰¾ä»£è¨€å’Œå“ç‰Œä¿¡æ¯ï¼ˆå¦‚æœçƒ­ç‚¹æ¶‰åŠçŸ¥åäººç‰©ï¼‰
            endorsement_info = None
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤3: æŸ¥æ‰¾ä»£è¨€å’Œå“ç‰Œä¿¡æ¯")
            
            # å°è¯•ä»æ ‡é¢˜ä¸­æå–äººç‰©åç§°ï¼ˆç®€å•å®ç°ï¼Œå¯ä»¥ä¼˜åŒ–ï¼‰
            # å¦‚æœæ ‡é¢˜åŒ…å«å¸¸è§çš„äººç‰©å…³é”®è¯ï¼Œå°è¯•æœç´¢ä»£è¨€ä¿¡æ¯
            person_keywords = ["ç‹æ¥šé’¦", "æ—é«˜è¿œ", "æ¨ŠæŒ¯ä¸œ", "ä½•æ°", "å¼ ä¼Ÿä¸½"]  # å¯ä»¥æ‰©å±•
            detected_person = None
            for keyword in person_keywords:
                if keyword in title:
                    detected_person = keyword
                    break
            
            if detected_person:
                try:
                    logger.info(f"ğŸ” [åŒ¹é…Agent] æ£€æµ‹åˆ°äººç‰©: {detected_person}ï¼ŒæŸ¥æ‰¾ä»£è¨€ä¿¡æ¯")
                    # è·å–ç›´æ’­é—´ç±»ç›®ç”¨äºè¿‡æ»¤
                    category = live_room_profile.split("ç±»ç›®ï¼š")[1].split("\n")[0] if "ç±»ç›®ï¼š" in live_room_profile else None
                    endorsement_info = search_endorsements(detected_person, category)
                    logger.info(f"âœ… [åŒ¹é…Agent] æ‰¾åˆ° {endorsement_info.get('total', 0)} æ¡ä»£è¨€ä¿¡æ¯")
                except Exception as e:
                    logger.warning(f"âš ï¸  [åŒ¹é…Agent] æŸ¥æ‰¾ä»£è¨€ä¿¡æ¯å¤±è´¥: {e}")
            else:
                logger.debug(f"ğŸ” [åŒ¹é…Agent] æœªæ£€æµ‹åˆ°çŸ¥åäººç‰©ï¼Œè·³è¿‡ä»£è¨€æœç´¢")
            
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤3å®Œæˆ: ä»£è¨€ä¿¡æ¯æŸ¥æ‰¾å®Œæˆ, è€—æ—¶ {step_time:.2f}ç§’")
            
            # 4. æ„å»ºåŒ¹é…æç¤ºè¯ï¼ˆåŒ…å«ä»£è¨€ä¿¡æ¯ï¼‰
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤4: æ„å»ºåŒ¹é…åˆ†ææç¤ºè¯")
            
            endorsement_text = ""
            if endorsement_info and endorsement_info.get("total", 0) > 0:
                endorsements = endorsement_info.get("endorsements", [])
                endorsement_text = "\n\n**ä»£è¨€å’Œå“ç‰Œä¿¡æ¯**ï¼š\n"
                for i, endo in enumerate(endorsements[:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                    endorsement_text += f"{i}. {endo.get('title', '')}\n   {endo.get('snippet', '')[:200]}...\n"
                endorsement_text += "\n**é‡è¦**ï¼šå¦‚æœæ‰¾åˆ°çš„ä»£è¨€å“ç‰Œä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…ï¼Œåº”è¯¥æå‡åŒ¹é…åº¦è¯„åˆ†ã€‚"
            
            # æå–é€‚ç”¨ç±»ç›®ä¿¡æ¯
            applicable_categories = ecommerce_fit.get("applicable_categories", [])
            applicable_categories_text = ""
            if applicable_categories:
                applicable_categories_text = f"\n- **é€‚ç”¨ç±»ç›®**ï¼ˆContentAnalysisAgentè¯†åˆ«ï¼‰ï¼š{', '.join(applicable_categories)}\n  âš ï¸ **é‡è¦**ï¼šå¿…é¡»æ£€æŸ¥è¿™äº›é€‚ç”¨ç±»ç›®æ˜¯å¦ä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…ã€‚å¦‚æœä¸åŒ¹é…ï¼Œåº”è¯¥å¤§å¹…é™ä½åŒ¹é…åº¦ã€‚"
            
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹çƒ­ç‚¹ä¸ç›´æ’­é—´çš„åŒ¹é…åº¦ï¼š

çƒ­ç‚¹ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{title}
- å†…å®¹æ‘˜è¦ï¼š{summary}
- è§†é¢‘é£æ ¼ï¼š{style}
- ç”µå•†é€‚é…æ€§è¯„åˆ†ï¼š{ecommerce_score:.2f}
- ç”µå•†é€‚é…æ€§åŸå› ï¼š{ecommerce_fit.get('reasoning', '')}
{applicable_categories_text}
- è§†é¢‘ç»“æ„ï¼š{str(video_structure)[:500] if video_structure else 'æ— '}
{endorsement_text}

ç›´æ’­é—´ç”»åƒï¼š
{live_room_profile}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡ŒåŒ¹é…åˆ†æï¼š
1. ä¸»é¢˜ç›¸å…³æ€§ï¼ˆ30%ï¼‰
2. å—ä¼—åŒ¹é…åº¦ï¼ˆ25%ï¼‰
3. é£æ ¼å¥‘åˆåº¦ï¼ˆ20%ï¼‰
4. å†…å®¹è½¬åŒ–æ½œåŠ›ï¼ˆ15%ï¼‰
5. é£é™©è¯„ä¼°ï¼ˆ10%ï¼‰

**ç‰¹åˆ«æ³¨æ„**ï¼š
- **é€‚ç”¨ç±»ç›®åŒ¹é…æ£€æŸ¥**ï¼ˆæœ€é‡è¦ï¼‰ï¼š
  - å¦‚æœContentAnalysisAgentè¯†åˆ«çš„é€‚ç”¨ç±»ç›®ä¸ç›´æ’­é—´ç±»ç›®å®Œå…¨ä¸åŒ¹é…ï¼ˆå¦‚"æ±½è½¦"vs"å®¶å±…"ã€"è¿åŠ¨é‹æœ"vs"å¥³è£…"ï¼‰ï¼Œåº”è¯¥å¤§å¹…é™ä½åŒ¹é…åº¦ï¼ˆä¸è¶…è¿‡0.4ï¼‰
  - å¦‚æœé€‚ç”¨ç±»ç›®ä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…ï¼Œå¯ä»¥æ­£å¸¸è®¡ç®—åŒ¹é…åº¦ï¼Œé€‚ç”¨ç±»ç›®åŒ¹é…æ˜¯é‡è¦çš„åŠ åˆ†é¡¹
  - é¿å…è¯¯åŒ¹é…ï¼š"å®¶ç”µ"ä¸åº”è¯¥åŒ¹é…"å®¶å±…å®¶è£…"ã€"è¿åŠ¨é‹æœ"ä¸åº”è¯¥åŒ¹é…"å¥³è£…"
- å¦‚æœæ‰¾åˆ°äº†ä»£è¨€å’Œå“ç‰Œä¿¡æ¯ï¼Œä¸”å“ç‰Œä¸ç›´æ’­é—´ç±»ç›®åŒ¹é…ï¼Œåº”è¯¥æå‡åŒ¹é…åº¦è¯„åˆ†
- ä»£è¨€ä¿¡æ¯å¯ä»¥ä½œä¸ºé¢å¤–çš„åŒ¹é…ä¾æ®ï¼Œè¯´æ˜çƒ­ç‚¹äººç‰©ä¸ç›´æ’­é—´ç±»ç›®æœ‰å•†ä¸šå…³è”

è¯·æä¾›ï¼š
- ç»¼åˆåŒ¹é…åº¦ï¼ˆ0-1ï¼‰
- å„ç»´åº¦è¯„åˆ†
- åŒ¹é…åŸå› 
- æ”¹è¿›å»ºè®®
"""
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤4å®Œæˆ: æç¤ºè¯æ„å»ºæˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] æç¤ºè¯é•¿åº¦: {len(analysis_prompt)}")
            
            # 5. è°ƒç”¨LLMè¿›è¡ŒåŒ¹é…åˆ†æ
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤5: è°ƒç”¨LLMè¿›è¡ŒåŒ¹é…åˆ†æ")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] è°ƒç”¨LLM: temperature=0.7, max_tokens=1000")
            response = await self.llm_client.generate(
                prompt=analysis_prompt,
                system_prompt=self._get_system_prompt(),
                temperature=0.7,
                max_tokens=1000
            )
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤5å®Œæˆ: LLMåˆ†ææˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
            
            analysis = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] LLMè¿”å›åˆ†æé•¿åº¦: {len(analysis)}")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] LLMåˆ†æå†…å®¹é¢„è§ˆ: {analysis[:200]}...")
            
            # 6. ä»åˆ†æä¸­æå–åŒ¹é…åº¦ï¼ˆç®€å•å®ç°ï¼šä½¿ç”¨ç”µå•†é€‚é…æ€§ä½œä¸ºåŸºç¡€ï¼‰
            step_start = time.time()
            logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤6: è®¡ç®—åŒ¹é…åº¦åˆ†æ•°")
            
            # å¦‚æœæ‰¾åˆ°ä»£è¨€ä¿¡æ¯ä¸”ä¸ç±»ç›®åŒ¹é…ï¼Œæå‡åŸºç¡€åˆ†æ•°
            endorsement_bonus = 0.0
            if endorsement_info and endorsement_info.get("total", 0) > 0:
                # æ£€æŸ¥ä»£è¨€ä¿¡æ¯ä¸­æ˜¯å¦åŒ…å«ç›´æ’­é—´ç±»ç›®ç›¸å…³çš„å“ç‰Œ
                category = live_room_profile.split("ç±»ç›®ï¼š")[1].split("\n")[0] if "ç±»ç›®ï¼š" in live_room_profile else ""
                if category:
                    for endo in endorsement_info.get("endorsements", []):
                        snippet = endo.get("snippet", "").lower()
                        title = endo.get("title", "").lower()
                        if category.lower() in snippet or category.lower() in title:
                            endorsement_bonus = 0.1  # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„ä»£è¨€ï¼Œé¢å¤–åŠ 10%
                            logger.info(f"âœ… [åŒ¹é…Agent] æ‰¾åˆ°åŒ¹é…çš„ä»£è¨€ä¿¡æ¯ï¼Œæå‡åŒ¹é…åº¦ {endorsement_bonus:.1%}")
                            break
            # å®é™…åº”è¯¥ä»LLMè¿”å›çš„ç»“æ„åŒ–æ•°æ®ä¸­æå–ï¼Œè¿™é‡Œå…ˆç®€åŒ–
            relevance_score = ecommerce_score * 0.7 + endorsement_bonus  # åŸºç¡€åˆ† + ä»£è¨€åŠ åˆ†
            logger.debug(f"ğŸ” [åŒ¹é…Agent] åŸºç¡€åŒ¹é…åº¦ï¼ˆåŸºäºç”µå•†é€‚é…æ€§ï¼‰: {relevance_score:.3f} = {ecommerce_score:.3f} * 0.7 + {endorsement_bonus:.3f}")
            
            # 6. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            hotspot_text = f"{title} {summary}"
            live_room_text = live_room_profile
            logger.debug(f"ğŸ” [åŒ¹é…Agent] è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦: çƒ­ç‚¹æ–‡æœ¬é•¿åº¦={len(hotspot_text)}, ç›´æ’­é—´æ–‡æœ¬é•¿åº¦={len(live_room_text)}")
            semantic_score = calculate_semantic_similarity(hotspot_text, live_room_text)
            logger.debug(f"ğŸ” [åŒ¹é…Agent] è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_score:.3f}")
            
            # ç»¼åˆåŒ¹é…åº¦
            final_score = (relevance_score * 0.6 + semantic_score * 0.4)
            logger.debug(f"ğŸ” [åŒ¹é…Agent] ç»¼åˆåŒ¹é…åº¦è®¡ç®—: {final_score:.3f} = {relevance_score:.3f} * 0.6 + {semantic_score:.3f} * 0.4")
            step_time = time.time() - step_start
            logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤6å®Œæˆ: åŒ¹é…åº¦è®¡ç®—æˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
            
            result = {
                "status": "success",
                "relevance_score": final_score,
                "semantic_score": semantic_score,
                "sentiment_score": 0.5,  # æš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
                "keyword_score": 0.0,  # æš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
                "analysis": analysis,
                "ecommerce_fit_score": ecommerce_score
            }
            
            total_time = time.time() - start_time
            logger.info(f"âœ… [åŒ¹é…Agent] åŒ¹é…åˆ†æå®Œæˆ, æ€»è€—æ—¶ {total_time:.2f}ç§’")
            logger.info(f"âœ… [åŒ¹é…Agent] æœ€ç»ˆåŒ¹é…åº¦: {final_score:.3f} (è¯­ä¹‰: {semantic_score:.3f}, ç”µå•†é€‚é…: {ecommerce_score:.3f})")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] å®Œæ•´ç»“æœ: {json.dumps(result, ensure_ascii=False)}")
            return result
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"âŒ [åŒ¹é…Agent] ä½¿ç”¨å®Œæ•´å†…å®¹åŒ…åˆ†æå¤±è´¥, è€—æ—¶ {total_time:.2f}ç§’: {e}")
            import traceback
            logger.error(f"âŒ [åŒ¹é…Agent] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            logger.warning(f"âš ï¸  [åŒ¹é…Agent] å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            return await self._execute_legacy({
                "hotspot_text": content_package.get("title", ""),
                "product_text": live_room_name
            })
    
    async def _execute_legacy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼ ç»Ÿæ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        import time
        start_time = time.time()
        
        hotspot_text = input_data.get("hotspot_text", "")
        product_text = input_data.get("product_text", "")
        
        if not hotspot_text or not product_text:
            raise ValueError("hotspot_textå’Œproduct_textä¸èƒ½ä¸ºç©º")
        
        logger.info(f"ğŸ” [åŒ¹é…Agent] å¼€å§‹å…³è”åº¦åˆ†æï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] çƒ­ç‚¹æ–‡æœ¬: {hotspot_text[:50]}...")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] å•†å“æ–‡æœ¬: {product_text[:50]}...")
        
        # 1. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        step_start = time.time()
        logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤1: è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦")
        semantic_score = calculate_semantic_similarity(hotspot_text, product_text)
        step_time = time.time() - step_start
        logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤1å®Œæˆ: è¯­ä¹‰ç›¸ä¼¼åº¦={semantic_score:.3f}, è€—æ—¶ {step_time:.2f}ç§’")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—è¯¦æƒ…: çƒ­ç‚¹æ–‡æœ¬é•¿åº¦={len(hotspot_text)}, å•†å“æ–‡æœ¬é•¿åº¦={len(product_text)}")
        
        # 2. åˆ†ææƒ…æ„Ÿ
        step_start = time.time()
        logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤2: åˆ†ææƒ…æ„ŸåŒ¹é…åº¦")
        hotspot_sentiment = analyze_sentiment(hotspot_text)
        product_sentiment = analyze_sentiment(product_text)
        logger.debug(f"ğŸ” [åŒ¹é…Agent] çƒ­ç‚¹æƒ…æ„Ÿ: {hotspot_sentiment}")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] å•†å“æƒ…æ„Ÿ: {product_sentiment}")
        
        # è®¡ç®—æƒ…æ„ŸåŒ¹é…åº¦ï¼ˆç®€å•å®ç°ï¼šå¦‚æœæƒ…æ„Ÿä¸€è‡´åˆ™é«˜åˆ†ï¼‰
        sentiment_match = 1.0 if hotspot_sentiment.get("sentiment") == product_sentiment.get("sentiment") else 0.5
        sentiment_score = (hotspot_sentiment.get("score", 0.5) + product_sentiment.get("score", 0.5)) / 2
        logger.debug(f"ğŸ” [åŒ¹é…Agent] æƒ…æ„ŸåŒ¹é…: {sentiment_match:.3f}, æƒ…æ„Ÿåˆ†æ•°: {sentiment_score:.3f}")
        step_time = time.time() - step_start
        logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤2å®Œæˆ: æƒ…æ„ŸåŒ¹é…åº¦={sentiment_score:.3f}, è€—æ—¶ {step_time:.2f}ç§’")
        
        # 3. å…³é”®è¯åŒ¹é…ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        step_start = time.time()
        logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤3: è®¡ç®—å…³é”®è¯åŒ¹é…åº¦")
        keyword_score = 0.0
        hotspot_tags = input_data.get("hotspot_tags", [])
        product_category = input_data.get("product_category", "")
        
        logger.debug(f"ğŸ” [åŒ¹é…Agent] çƒ­ç‚¹æ ‡ç­¾: {hotspot_tags}")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] å•†å“ç±»ç›®: {product_category}")
        
        if hotspot_tags and product_category:
            # æ£€æŸ¥æ ‡ç­¾ä¸­æ˜¯å¦åŒ…å«ç±»ç›®å…³é”®è¯
            if product_category in hotspot_tags:
                keyword_score = 1.0
                logger.debug(f"ğŸ” [åŒ¹é…Agent] å…³é”®è¯å®Œå…¨åŒ¹é…: {product_category} åœ¨æ ‡ç­¾ä¸­")
            else:
                # éƒ¨åˆ†åŒ¹é…
                keyword_score = 0.3
                logger.debug(f"ğŸ” [åŒ¹é…Agent] å…³é”®è¯éƒ¨åˆ†åŒ¹é…: {product_category} ä¸åœ¨æ ‡ç­¾ä¸­")
        else:
            logger.debug(f"ğŸ” [åŒ¹é…Agent] å…³é”®è¯åŒ¹é…è·³è¿‡: ç¼ºå°‘æ ‡ç­¾æˆ–ç±»ç›®ä¿¡æ¯")
        
        step_time = time.time() - step_start
        logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤3å®Œæˆ: å…³é”®è¯åŒ¹é…åº¦={keyword_score:.3f}, è€—æ—¶ {step_time:.2f}ç§’")
        
        # 4. ç»¼åˆè®¡ç®—åŒ¹é…åº¦
        step_start = time.time()
        logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤4: ç»¼åˆè®¡ç®—åŒ¹é…åº¦")
        relevance_score = (
            semantic_score * 0.6 +
            sentiment_score * 0.3 +
            keyword_score * 0.1
        )
        logger.debug(f"ğŸ” [åŒ¹é…Agent] ç»¼åˆåŒ¹é…åº¦è®¡ç®—: {relevance_score:.3f} = {semantic_score:.3f} * 0.6 + {sentiment_score:.3f} * 0.3 + {keyword_score:.3f} * 0.1")
        step_time = time.time() - step_start
        logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤4å®Œæˆ: ç»¼åˆåŒ¹é…åº¦={relevance_score:.3f}, è€—æ—¶ {step_time:.2f}ç§’")
        
        # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„å…³è”åº¦ï¼š

çƒ­ç‚¹ï¼š{hotspot_text}
å•†å“ï¼š{product_text}

è®¡ç®—ç»“æœï¼š
- è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š{semantic_score:.2f}
- æƒ…æ„ŸåŒ¹é…åº¦ï¼š{sentiment_score:.2f}
- å…³é”®è¯åŒ¹é…ï¼š{keyword_score:.2f}
- ç»¼åˆåŒ¹é…åº¦ï¼š{relevance_score:.2f}

è¯·æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
1. å…³è”åº¦è¯„ä¼°
2. åŒ¹é…åŸå› 
3. æ”¹è¿›å»ºè®®
"""
        
        # è°ƒç”¨LLMç”Ÿæˆåˆ†ææŠ¥å‘Š
        step_start = time.time()
        logger.info(f"ğŸ” [åŒ¹é…Agent] æ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        try:
            logger.debug(f"ğŸ” [åŒ¹é…Agent] è°ƒç”¨LLMç”Ÿæˆåˆ†ææŠ¥å‘Š: temperature=0.7, max_tokens=500")
            response = await self.llm_client.generate(
                prompt=analysis_prompt,
                system_prompt=self._get_system_prompt(),
                temperature=0.7,
                max_tokens=500
            )
            analysis = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            logger.debug(f"ğŸ” [åŒ¹é…Agent] LLMåˆ†ææŠ¥å‘Šé•¿åº¦: {len(analysis)}")
        except Exception as e:
            logger.error(f"âŒ [åŒ¹é…Agent] ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            analysis = f"åŒ¹é…åº¦åˆ†æï¼š{relevance_score:.2f}ï¼ˆè¯­ä¹‰ï¼š{semantic_score:.2f}ï¼Œæƒ…æ„Ÿï¼š{sentiment_score:.2f}ï¼‰"
        step_time = time.time() - step_start
        logger.info(f"âœ… [åŒ¹é…Agent] æ­¥éª¤5å®Œæˆ: åˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’")
        
        result = {
            "status": "success",
            "relevance_score": relevance_score,
            "semantic_score": semantic_score,
            "sentiment_score": sentiment_score,
            "keyword_score": keyword_score,
            "analysis": analysis,
            "hotspot_sentiment": hotspot_sentiment,
            "product_sentiment": product_sentiment
        }
        
        total_time = time.time() - start_time
        logger.info(f"âœ… [åŒ¹é…Agent] å…³è”åº¦åˆ†æå®Œæˆ, æ€»è€—æ—¶ {total_time:.2f}ç§’")
        logger.info(f"âœ… [åŒ¹é…Agent] æœ€ç»ˆåŒ¹é…åº¦: {relevance_score:.3f} (è¯­ä¹‰: {semantic_score:.3f}, æƒ…æ„Ÿ: {sentiment_score:.3f}, å…³é”®è¯: {keyword_score:.3f})")
        logger.debug(f"ğŸ” [åŒ¹é…Agent] å®Œæ•´ç»“æœ: {json.dumps(result, ensure_ascii=False, default=str)}")
        return result

