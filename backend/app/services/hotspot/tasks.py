"""
çƒ­ç‚¹ç›‘æ§å®šæ—¶ä»»åŠ¡
"""
import asyncio
import json
from datetime import datetime
from pathlib import Path
from app.celery_app import celery_app
from app.core.database import SessionLocal
from app.services.hotspot.service import HotspotMonitorService
from loguru import logger


@celery_app.task(bind=True)
def fetch_daily_hotspots(self, platform: str = None, live_room_id: str = None):
    """
    æ¯æ—¥8:00è‡ªåŠ¨æŠ“å–çƒ­ç‚¹ï¼ˆä½¿ç”¨è¯­ä¹‰ç­›é€‰ï¼‰
    
    Args:
        platform: å¹³å°æ ‡è¯†ï¼Œå¦‚æœä¸ºNoneåˆ™æŠ“å–å¤šä¸ªå¹³å°
        live_room_id: ç›´æ’­é—´ID
    """
    # å®šä¹‰è¦æŠ“å–çš„å¹³å°åˆ—è¡¨ï¼ˆæ¯ä¸ªå¹³å°30ä¸ªçƒ­ç‚¹ï¼‰
    if platform:
        platforms = [platform]
    else:
        # é»˜è®¤æŠ“å–å¤šä¸ªä¸»æµå¹³å°
        platforms = ["douyin", "zhihu", "weibo", "bilibili"]
    
    logger.info(f"å¼€å§‹æŠ“å–æ¯æ—¥çƒ­ç‚¹ï¼Œå¹³å°: {platforms}, ç›´æ’­é—´: {live_room_id}")
    
    # æ›´æ–°çŠ¶æ€ï¼šå¼€å§‹æŠ“å–
    self.update_state(
        state='PROGRESS',
        meta={
            'current': 0,
            'total': len(platforms),
            'status': f'å¼€å§‹æŠ“å– {len(platforms)} ä¸ªå¹³å°çš„çƒ­ç‚¹...'
        }
    )
    
    try:
        service = HotspotMonitorService()
        db = SessionLocal()
        
        try:
            # å¼‚æ­¥è·å–æ‰€æœ‰å¹³å°çš„çƒ­ç‚¹
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            all_hotspots = []
            platform_counts = {}
            
            # å¹¶å‘æŠ“å–å¤šä¸ªå¹³å°
            fetched_count = 0
            fetch_lock = asyncio.Lock()
            
            async def fetch_platform(platform_name):
                nonlocal fetched_count
                try:
                    hotspots = await service.fetch_hotspots(platform=platform_name)
                    
                    # ä½¿ç”¨é”ä¿æŠ¤è®¡æ•°å™¨æ›´æ–°
                    async with fetch_lock:
                        fetched_count += 1
                        current_count = fetched_count
                    
                    logger.info(f"å¹³å° {platform_name} æŠ“å–åˆ° {len(hotspots)} ä¸ªçƒ­ç‚¹")
                    
                    # æ›´æ–°è¿›åº¦ï¼šæŠ“å–å¹³å°è¿›åº¦
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': current_count,
                            'total': len(platforms),
                            'status': f'å·²æŠ“å– {current_count}/{len(platforms)} ä¸ªå¹³å°ï¼Œ{platform_name} æŠ“å–åˆ° {len(hotspots)} ä¸ªçƒ­ç‚¹'
                        }
                    )
                    
                    return platform_name, hotspots
                except Exception as e:
                    # ä½¿ç”¨é”ä¿æŠ¤è®¡æ•°å™¨æ›´æ–°
                    async with fetch_lock:
                        fetched_count += 1
                        current_count = fetched_count
                    
                    logger.error(f"å¹³å° {platform_name} æŠ“å–å¤±è´¥: {e}")
                    
                    # æ›´æ–°è¿›åº¦ï¼šæŠ“å–å¤±è´¥
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': current_count,
                            'total': len(platforms),
                            'status': f'å·²æŠ“å– {current_count}/{len(platforms)} ä¸ªå¹³å°ï¼Œ{platform_name} æŠ“å–å¤±è´¥'
                        }
                    )
                    
                    return platform_name, []
            
            # å¹¶å‘æŠ“å–æ‰€æœ‰å¹³å°
            results = loop.run_until_complete(
                asyncio.gather(*[fetch_platform(p) for p in platforms])
            )
            
            # æ±‡æ€»æ‰€æœ‰å¹³å°çš„çƒ­ç‚¹
            for platform_name, hotspots in results:
                all_hotspots.extend(hotspots)
                platform_counts[platform_name] = len(hotspots)
            
            logger.info(f"æ€»å…±æŠ“å–åˆ° {len(all_hotspots)} ä¸ªçƒ­ç‚¹ï¼ˆæ¥è‡ª {len(platforms)} ä¸ªå¹³å°ï¼‰")
            
            # æ›´æ–°çŠ¶æ€ï¼šæŠ“å–å®Œæˆï¼Œå¼€å§‹è¯­ä¹‰ç­›é€‰
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': len(platforms),
                    'total': len(platforms) + 1,
                    'status': f'æŠ“å–å®Œæˆï¼å…±æŠ“å– {len(all_hotspots)} ä¸ªçƒ­ç‚¹ï¼Œå¼€å§‹è¯­ä¹‰ç­›é€‰...'
                }
            )
            
            # ä½¿ç”¨è¯­ä¹‰å…³è”åº¦ç­›é€‰çƒ­ç‚¹
            from datetime import datetime
            filtered_hotspots = loop.run_until_complete(
                service.filter_hotspots_with_semantic(
                    db, all_hotspots, live_room_id=live_room_id, target_date=datetime.now()
                )
            )
            
            # æ›´æ–°çŠ¶æ€ï¼šè¯­ä¹‰ç­›é€‰å®Œæˆ
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': len(platforms) + 1,
                    'total': len(platforms) + 1 + (1 if filtered_hotspots else 0),
                    'status': f'è¯­ä¹‰ç­›é€‰å®Œæˆï¼Œå‰©ä½™ {len(filtered_hotspots)} ä¸ªçƒ­ç‚¹'
                }
            )
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥filtered_hotspotsçš„å®é™…è¿”å›å€¼
            logger.info(f"[DEBUG] filter_hotspots_with_semanticè¿”å›: type={type(filtered_hotspots)}, len={len(filtered_hotspots) if filtered_hotspots else 'N/A'}, bool={bool(filtered_hotspots)}")
            if filtered_hotspots:
                logger.info(f"[DEBUG] filtered_hotspotså‰3ä¸ª: {filtered_hotspots[:3] if len(filtered_hotspots) >= 3 else filtered_hotspots}")
            else:
                logger.warning(f"[DEBUG] filtered_hotspotsä¸ºç©ºæˆ–Noneï¼Œæ— æ³•æ‰§è¡Œå¢å¼ºé€»è¾‘")
            
            # æ–°å¢ï¼šä½¿ç”¨ContentStructureAgentå’ŒContentAnalysisAgentå¢å¼ºçƒ­ç‚¹ä¿¡æ¯
            if filtered_hotspots:
                logger.info("[ENRICH] ä½¿ç”¨ContentStructureAgentå’ŒContentAnalysisAgentå¢å¼ºçƒ­ç‚¹ä¿¡æ¯...")
                try:
                    from app.agents import get_content_structure_agent, get_content_analysis_agent
                    
                    structure_agent = get_content_structure_agent()
                    analysis_agent = get_content_analysis_agent()
                    logger.info(f"[ENRICH] Agentåˆå§‹åŒ–æˆåŠŸï¼Œå‡†å¤‡å¢å¼º {len(filtered_hotspots)} ä¸ªçƒ­ç‚¹ï¼ˆä½¿ç”¨æœ¬åœ°è§†é¢‘åˆ†æå·¥å…·åŒ…ï¼‰")
                except Exception as e:
                    logger.error(f"[ENRICH] Agentåˆå§‹åŒ–å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    raise
                
                # å¢å¼ºæ‰€æœ‰çƒ­ç‚¹ï¼ˆä½¿ç”¨æœ¬åœ°å·¥å…·åŒ…ï¼Œæ— APIæˆæœ¬é™åˆ¶ï¼‰
                # æ³¨æ„ï¼šå¢å¼ºæ‰€æœ‰ç­›é€‰åçš„çƒ­ç‚¹ï¼Œä¸é™åˆ¶æ•°é‡
                hotspots_to_enrich = filtered_hotspots
                logger.info(f"[ENRICH] å‡†å¤‡å¢å¼º {len(hotspots_to_enrich)} ä¸ªçƒ­ç‚¹ï¼ˆå…¨éƒ¨ç­›é€‰åçš„çƒ­ç‚¹ï¼‰")
                
                # æ›´æ–°çŠ¶æ€ï¼šå¼€å§‹å¢å¼º
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': len(platforms) + 1,
                        'total': len(platforms) + 1 + len(hotspots_to_enrich),
                        'status': f'å¼€å§‹è§£æ {len(hotspots_to_enrich)} ä¸ªçƒ­ç‚¹...'
                    }
                )
                
                enriched_count = 0
                enrich_lock = asyncio.Lock()
                
                async def enrich_hotspot(hotspot):
                    """å¢å¼ºå•ä¸ªçƒ­ç‚¹"""
                    import time
                    hotspot_start = time.time()
                    
                    url = hotspot.get("url")
                    title = hotspot.get("title", "")
                    hotspot_id = hotspot.get("id") or hotspot.get("title", "unknown")[:50]
                    
                    logger.info(f"ğŸ” [æ¢é’ˆ] enrich_hotspot å¼€å§‹: {hotspot_id}")
                    logger.debug(f"ğŸ” [æ¢é’ˆ] çƒ­ç‚¹ä¿¡æ¯: url={url[:100] if url else 'N/A'}, title={title[:50] if title else 'N/A'}")
                    
                    try:
                        if not url:
                            logger.warning(f"âš ï¸  [æ¢é’ˆ] çƒ­ç‚¹æ— URLï¼Œè·³è¿‡å¢å¼º: {hotspot_id}")
                            hotspot["enrichment_skipped"] = True
                            hotspot["enrichment_reason"] = "æ— URL"
                            return hotspot
                        
                        # 1. ä½¿ç”¨ContentStructureAgentæå–è§†é¢‘ç»“æ„
                        step_start = time.time()
                        logger.info(f"ğŸ” [æ¢é’ˆ] æ­¥éª¤1: æå–è§†é¢‘ç»“æ„ - {hotspot_id}")
                        logger.debug(f"æå–è§†é¢‘ç»“æ„: {title[:50]}")
                        structure_result = await structure_agent.execute({
                            "url": url,
                            "title": title
                        })
                        step_time = time.time() - step_start
                        logger.info(f"âœ… [æ¢é’ˆ] æ­¥éª¤1å®Œæˆ: è§†é¢‘ç»“æ„æå–æˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’ - {hotspot_id}")
                        
                        video_structure = structure_result.get("video_structure", {})
                        hotspot["video_structure"] = video_structure
                        logger.debug(f"ğŸ” [æ¢é’ˆ] è§†é¢‘ç»“æ„æ‘˜è¦: duration={video_structure.get('duration')}, scenes={len(video_structure.get('scenes', []))}, transcript_len={len(video_structure.get('transcript', ''))}")
                        
                        # 2. ä½¿ç”¨ContentAnalysisAgentåˆ†æå†…å®¹
                        step_start = time.time()
                        logger.info(f"ğŸ” [æ¢é’ˆ] æ­¥éª¤2: åˆ†æè§†é¢‘å†…å®¹ - {hotspot_id}")
                        logger.debug(f"åˆ†æè§†é¢‘å†…å®¹: {title[:50]}")
                        analysis_result = await analysis_agent.execute({
                            "video_structure": video_structure,
                            "title": title,
                            "url": url
                        })
                        step_time = time.time() - step_start
                        logger.info(f"âœ… [æ¢é’ˆ] æ­¥éª¤2å®Œæˆ: å†…å®¹åˆ†ææˆåŠŸ, è€—æ—¶ {step_time:.2f}ç§’ - {hotspot_id}")
                        
                        content_analysis = analysis_result.get("content_analysis", {})
                        hotspot["content_analysis"] = content_analysis
                        logger.debug(f"ğŸ” [æ¢é’ˆ] å†…å®¹åˆ†ææ‘˜è¦: summary_len={len(content_analysis.get('summary', ''))}")
                        
                        # 3. æå–å†…å®¹æ‘˜è¦ï¼ˆç”¨äºcontent_compactå­—æ®µï¼‰
                        step_start = time.time()
                        logger.info(f"ğŸ” [æ¢é’ˆ] æ­¥éª¤3: æå–å†…å®¹æ‘˜è¦ - {hotspot_id}")
                        summary = content_analysis.get("summary", "")
                        if summary:
                            hotspot["content_compact"] = summary
                            logger.debug(f"ğŸ” [æ¢é’ˆ] ä½¿ç”¨åˆ†ææ‘˜è¦ä½œä¸ºcontent_compact: é•¿åº¦={len(summary)}")
                        elif video_structure.get("transcript"):
                            hotspot["content_compact"] = video_structure.get("transcript", "")[:500]
                            logger.debug(f"ğŸ” [æ¢é’ˆ] ä½¿ç”¨è½¬å½•æ–‡æœ¬ä½œä¸ºcontent_compact: é•¿åº¦={len(hotspot['content_compact'])}")
                        else:
                            logger.warning(f"âš ï¸  [æ¢é’ˆ] æ— å¯ç”¨å†…å®¹ä½œä¸ºcontent_compact - {hotspot_id}")
                        
                        step_time = time.time() - step_start
                        total_time = time.time() - hotspot_start
                        logger.info(f"âœ… [æ¢é’ˆ] æ­¥éª¤3å®Œæˆ: å†…å®¹æ‘˜è¦æå–æˆåŠŸ, è€—æ—¶ {step_time:.4f}ç§’ - {hotspot_id}")
                        logger.info(f"âœ… [æ¢é’ˆ] enrich_hotspot å®Œæˆ, æ€»è€—æ—¶ {total_time:.2f}ç§’ - {hotspot_id}")
                        logger.debug(f"çƒ­ç‚¹å¢å¼ºå®Œæˆ: {title[:50]}")
                        return hotspot
                    except Exception as e:
                        total_time = time.time() - hotspot_start
                        logger.error(f"âŒ [æ¢é’ˆ] enrich_hotspot å¤±è´¥, è€—æ—¶ {total_time:.2f}ç§’ - {hotspot_id}: {e}")
                        import traceback
                        logger.debug(f"âŒ [æ¢é’ˆ] é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                        logger.warning(f"çƒ­ç‚¹å¢å¼ºå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹çƒ­ç‚¹")
                        return hotspot
                
                # å¹¶å‘å¢å¼ºçƒ­ç‚¹ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
                semaphore = asyncio.Semaphore(3)  # æœ€å¤š3ä¸ªå¹¶å‘
                
                async def enrich_with_semaphore(hotspot):
                    nonlocal enriched_count
                    async with semaphore:
                        result = await enrich_hotspot(hotspot)
                        
                        # ä½¿ç”¨é”ä¿æŠ¤è®¡æ•°å™¨æ›´æ–°
                        async with enrich_lock:
                            enriched_count += 1
                            current_count = enriched_count
                        
                        # æ›´æ–°è¿›åº¦ï¼šå¢å¼ºè¿›åº¦
                        self.update_state(
                            state='PROGRESS',
                            meta={
                                'current': len(platforms) + 1 + current_count,
                                'total': len(platforms) + 1 + len(hotspots_to_enrich),
                                'status': f'æ­£åœ¨è§£æçƒ­ç‚¹: {current_count}/{len(hotspots_to_enrich)}'
                            }
                        )
                        
                        return result
                
                enriched_hotspots = loop.run_until_complete(
                    asyncio.gather(*[enrich_with_semaphore(h) for h in hotspots_to_enrich])
                )
                
                # åˆå¹¶å¢å¼ºåçš„çƒ­ç‚¹ï¼ˆæ›¿æ¢æ‰€æœ‰çƒ­ç‚¹ï¼‰
                filtered_hotspots = enriched_hotspots
                logger.info(f"[ENRICH] æˆåŠŸå¢å¼º {len(enriched_hotspots)} ä¸ªçƒ­ç‚¹ï¼ˆå…± {len(filtered_hotspots)} ä¸ªï¼‰")
                
                # æ›´æ–°çŠ¶æ€ï¼šå¢å¼ºå®Œæˆ
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': len(platforms) + 1 + len(enriched_hotspots),
                        'total': len(platforms) + 1 + len(hotspots_to_enrich),
                        'status': f'è§£æå®Œæˆï¼å·²è§£æ {len(enriched_hotspots)} ä¸ªçƒ­ç‚¹ï¼Œæ­£åœ¨ä¿å­˜...'
                    }
                )
                # è°ƒè¯•ï¼šæ£€æŸ¥å¢å¼ºåçš„æ•°æ®
                for i, h in enumerate(enriched_hotspots[:3], 1):
                    logger.debug(f"[ENRICH] çƒ­ç‚¹#{i}: video_structure={'æœ‰' if h.get('video_structure') else 'æ— '}, content_analysis={'æœ‰' if h.get('content_analysis') else 'æ— '}, content_compact={'æœ‰' if h.get('content_compact') else 'æ— '}")
                
                # ä¿å­˜agentsè¾“å‡ºç»“æœåˆ°upgrade.md
                try:
                    save_agents_output_to_upgrade_md(enriched_hotspots)
                    logger.info("[ENRICH] å·²ä¿å­˜agentsè¾“å‡ºç»“æœåˆ°upgrade.md")
                except Exception as e:
                    logger.error(f"[ENRICH] ä¿å­˜agentsè¾“å‡ºåˆ°upgrade.mdå¤±è´¥: {e}")
            
            # Firecrawlå¢å¼ºå·²ç§»é™¤ï¼šä¸éœ€è¦Firecrawlï¼ŒContentStructureAgentå’ŒContentAnalysisAgentå·²ç»è¶³å¤Ÿ
            # å¦‚æœå°†æ¥éœ€è¦ï¼Œå¯ä»¥é€šè¿‡é…ç½®FIRECRAWL_ENABLEDé‡æ–°å¯ç”¨
            
            try:
                loop.close()
            except:
                pass
            
            if filtered_hotspots:
                # æŒ‰å¹³å°åˆ†ç»„ä¿å­˜
                from collections import defaultdict
                hotspots_by_platform = defaultdict(list)
                for hotspot in filtered_hotspots:
                    platform_name = hotspot.get("platform", "unknown")
                    hotspots_by_platform[platform_name].append(hotspot)
                
                # æ›´æ–°çŠ¶æ€ï¼šå¼€å§‹ä¿å­˜
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': len(platforms) + 1 + len(enriched_hotspots) if filtered_hotspots and any(h.get('video_structure') or h.get('content_analysis') for h in filtered_hotspots) else len(platforms) + 1,
                        'total': len(platforms) + 1 + len(hotspots_to_enrich) if filtered_hotspots and any(h.get('video_structure') or h.get('content_analysis') for h in filtered_hotspots) else len(platforms) + 1,
                        'status': f'æ­£åœ¨ä¿å­˜ {len(filtered_hotspots)} ä¸ªçƒ­ç‚¹åˆ°æ•°æ®åº“...'
                    }
                )
                
                # ä¿å­˜æ¯ä¸ªå¹³å°çš„çƒ­ç‚¹
                total_saved = 0
                for platform_name, platform_hotspots in hotspots_by_platform.items():
                    saved = service.save_hotspots(db, platform_hotspots, platform_name)
                    total_saved += saved
                    logger.info(f"å¹³å° {platform_name} ä¿å­˜äº† {saved} ä¸ªçƒ­ç‚¹")
                
                logger.info(f"æˆåŠŸæŠ“å–å¹¶ä¿å­˜ {total_saved} ä¸ªçƒ­ç‚¹ï¼ˆæ¥è‡ª {len(platforms)} ä¸ªå¹³å°ï¼Œè¯­ä¹‰ç­›é€‰åï¼‰")
                
                # æ›´æ–°çŠ¶æ€ï¼šä¿å­˜å®Œæˆ
                final_total = len(platforms) + 1 + len(hotspots_to_enrich) if filtered_hotspots and any(h.get('video_structure') or h.get('content_analysis') for h in filtered_hotspots) else len(platforms) + 1
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': final_total,
                        'total': final_total,
                        'status': f'ä¿å­˜å®Œæˆï¼å…±ä¿å­˜ {total_saved} ä¸ªçƒ­ç‚¹'
                    }
                )
            else:
                logger.warning(f"è¯­ä¹‰ç­›é€‰åæ²¡æœ‰çƒ­ç‚¹ï¼ˆåŸå§‹çƒ­ç‚¹æ•°: {len(all_hotspots)}ï¼‰")
            
            # ä»»åŠ¡å®Œæˆï¼Œè¿”å›SUCCESSçŠ¶æ€
            return {
                "status": "success",
                "message": f"çƒ­ç‚¹æŠ“å–ä»»åŠ¡å·²å®Œæˆï¼ˆ{len(platforms)} ä¸ªå¹³å°ï¼Œè¯­ä¹‰ç­›é€‰åï¼‰",
                "count": len(filtered_hotspots) if filtered_hotspots else 0,
                "platforms": platform_counts
            }
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"çƒ­ç‚¹æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


@celery_app.task
def push_hotspots_to_feishu(live_room_id: str = None):
    """æ¯æ—¥9:00æ¨é€çƒ­ç‚¹åˆ°é£ä¹¦"""
    logger.info("å¼€å§‹æ¨é€çƒ­ç‚¹åˆ°é£ä¹¦")
    
    try:
        service = HotspotMonitorService()
        db = SessionLocal()
        
        try:
            # å¼‚æ­¥æ¨é€
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            success = loop.run_until_complete(
                service.push_to_feishu(db, live_room_id)
            )
            loop.close()
            
            if success:
                logger.info("çƒ­ç‚¹æ¨é€å®Œæˆ")
                return {"status": "success", "message": "çƒ­ç‚¹æ¨é€ä»»åŠ¡å·²å®Œæˆ"}
            else:
                logger.warning("çƒ­ç‚¹æ¨é€å¤±è´¥")
                return {"status": "error", "message": "çƒ­ç‚¹æ¨é€å¤±è´¥"}
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"æ¨é€çƒ­ç‚¹åˆ°é£ä¹¦å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}


def save_agents_output_to_upgrade_md(enriched_hotspots: list):
    """
    ä¿å­˜agentsè¾“å‡ºç»“æœåˆ°upgrade.mdæ–‡ä»¶
    
    Args:
        enriched_hotspots: å¢å¼ºåçš„çƒ­ç‚¹åˆ—è¡¨
    """
    try:
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent.parent
        upgrade_md_path = project_root / "upgrade.md"
        
        if not upgrade_md_path.exists():
            logger.warning(f"upgrade.mdæ–‡ä»¶ä¸å­˜åœ¨: {upgrade_md_path}")
            return
        
        # è¯»å–ç°æœ‰å†…å®¹
        with open(upgrade_md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾"## äºŒã€30ä¸ªçƒ­ç‚¹è¯¦ç»†åˆ—è¡¨"éƒ¨åˆ†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        section_marker = "## äºŒã€30ä¸ªçƒ­ç‚¹è¯¦ç»†åˆ—è¡¨"
        if section_marker not in content:
            # å¦‚æœä¸å­˜åœ¨ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
            new_section = f"\n\n{section_marker}\n\n"
            content += new_section
        
        # ç”Ÿæˆæ–°çš„agentsè¾“å‡ºå†…å®¹
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        agents_output = f"\n\n### Agentsè¾“å‡ºç»“æœï¼ˆæ›´æ–°æ—¶é—´: {timestamp}ï¼‰\n\n"
        agents_output += f"æœ¬æ¬¡å…±å¤„ç† {len(enriched_hotspots)} ä¸ªçƒ­ç‚¹\n\n"
        
        for idx, hotspot in enumerate(enriched_hotspots, 1):
            title = hotspot.get("title", "æœªçŸ¥æ ‡é¢˜")
            url = hotspot.get("url", "")
            platform = hotspot.get("platform", "unknown")
            
            agents_output += f"#### {idx}. {title}\n"
            agents_output += f"- **URL**: {url}\n"
            agents_output += f"- **å¹³å°**: {platform}\n"
            
            # ContentStructureAgentè¾“å‡º
            video_structure = hotspot.get("video_structure", {})
            if video_structure:
                agents_output += f"- **ContentStructureAgentè¾“å‡º**:\n"
                agents_output += f"  - è§†é¢‘æ—¶é•¿: {video_structure.get('duration', 0.0)}ç§’\n"
                agents_output += f"  - åœºæ™¯æ•°: {len(video_structure.get('scenes', []))}\n"
                agents_output += f"  - å…³é”®å¸§æ•°: {len(video_structure.get('key_frames', []))}\n"
                agents_output += f"  - è½¬å½•æ–‡æœ¬é•¿åº¦: {len(video_structure.get('transcript', ''))}\n"
                agents_output += f"  - è§†è§‰å…ƒç´ : {json.dumps(video_structure.get('visual_elements', {}), ensure_ascii=False)[:200]}...\n"
                agents_output += f"  - éŸ³é¢‘å…ƒç´ : {json.dumps(video_structure.get('audio_elements', {}), ensure_ascii=False)[:200]}...\n"
            else:
                agents_output += f"- **ContentStructureAgentè¾“å‡º**: æ— \n"
            
            # ContentAnalysisAgentè¾“å‡º
            content_analysis = hotspot.get("content_analysis", {})
            if content_analysis:
                agents_output += f"- **ContentAnalysisAgentè¾“å‡º**:\n"
                agents_output += f"  - å†…å®¹æ‘˜è¦: {content_analysis.get('summary', 'æ— ')}\n"
                agents_output += f"  - è§†é¢‘é£æ ¼: {content_analysis.get('style', 'æ— ')}\n"
                ecommerce_fit = content_analysis.get("ecommerce_fit", {})
                if ecommerce_fit:
                    agents_output += f"  - ç”µå•†é€‚é…æ€§è¯„åˆ†: {ecommerce_fit.get('score', 0.0)}\n"
                    agents_output += f"  - é€‚é…æ€§åŸå› : {ecommerce_fit.get('reasoning', 'æ— ')}\n"
                    agents_output += f"  - é€‚ç”¨ç±»ç›®: {', '.join(ecommerce_fit.get('applicable_categories', []))}\n"
                script_structure = content_analysis.get("script_structure", {})
                if script_structure:
                    agents_output += f"  - è„šæœ¬ç»“æ„: {json.dumps(script_structure, ensure_ascii=False)}\n"
            else:
                agents_output += f"- **ContentAnalysisAgentè¾“å‡º**: æ— \n"
            
            agents_output += "\n"
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢æˆ–è¿½åŠ å†…å®¹
        # å¦‚æœå­˜åœ¨"### Agentsè¾“å‡ºç»“æœ"éƒ¨åˆ†ï¼Œåˆ™æ›¿æ¢ï¼›å¦åˆ™è¿½åŠ 
        agents_section_marker = "### Agentsè¾“å‡ºç»“æœ"
        if agents_section_marker in content:
            # æ‰¾åˆ°æœ€åä¸€ä¸ª"### Agentsè¾“å‡ºç»“æœ"çš„ä½ç½®
            last_pos = content.rfind(agents_section_marker)
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ª"##"æˆ–æ–‡ä»¶æœ«å°¾
            next_section_pos = content.find("\n## ", last_pos)
            if next_section_pos == -1:
                # æ²¡æœ‰ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œæ›¿æ¢åˆ°æ–‡ä»¶æœ«å°¾
                content = content[:last_pos] + agents_output
            else:
                # æ›¿æ¢åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚ä¹‹å‰
                content = content[:last_pos] + agents_output + content[next_section_pos:]
        else:
            # è¿½åŠ åˆ°"## äºŒã€30ä¸ªçƒ­ç‚¹è¯¦ç»†åˆ—è¡¨"éƒ¨åˆ†ä¹‹å
            section_pos = content.find(section_marker)
            if section_pos != -1:
                # æ‰¾åˆ°è¯¥ç« èŠ‚çš„ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ª"##"æˆ–æ–‡ä»¶æœ«å°¾ï¼‰
                next_section_pos = content.find("\n## ", section_pos + len(section_marker))
                if next_section_pos == -1:
                    # æ²¡æœ‰ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œè¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
                    content = content + agents_output
                else:
                    # æ’å…¥åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚ä¹‹å‰
                    content = content[:next_section_pos] + agents_output + content[next_section_pos:]
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ç« èŠ‚ï¼Œè¿½åŠ åˆ°æ–‡ä»¶æœ«å°¾
                content = content + agents_output
        
        # å†™å›æ–‡ä»¶
        with open(upgrade_md_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"å·²æ›´æ–°upgrade.mdæ–‡ä»¶: {upgrade_md_path}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜agentsè¾“å‡ºåˆ°upgrade.mdå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())

